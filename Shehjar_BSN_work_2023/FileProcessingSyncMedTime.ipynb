{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155bbe64-7ae4-4052-9927-527f87207a48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1839684-a2ec-4f73-b09d-abf3b35f6726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dset_path = \"/Users/shehjarsadhu/Desktop/UniversityOfRhodeIsland/Graduate/WBL/Project_IOTEX/iotex-glove/PD/\"\n",
    "# file_id = []\n",
    "for i in range(1):\n",
    "    pno = i+1\n",
    "    print(\"Part No: {}\".format(pno))\n",
    "    # csv_files = glob.glob('C:/Users/dan95/Desktop/Participant1/**/*.csv',recursive=True)\n",
    "    dset_csv_fpath = dset_path +'/Participant'+str(pno)+'/**/*.csv'\n",
    "    # print(\"dset_csv_fpath: \",dset_csv_fpath)\n",
    "    label_text_fpath = dset_path +'/Participant'+str(pno)+ \"/**/*.txt\"\n",
    "    print(\"label_text_fpath: \", label_text_fpath)\n",
    "    csv_files = glob.glob(dset_csv_fpath,recursive=True)\n",
    "    #print(\"csv_files: \",csv_files)\n",
    "    text_files = glob.glob(label_text_fpath,recursive=True)\n",
    "    rg_files = [s for s in csv_files if \"rg_\" in s]\n",
    "    lg_files = [s for s in csv_files if \"lg_\" in s]\n",
    "    #print(\"rg_files: \",rg_files)\n",
    "    #print(\"len(rg_files),rg_files: \",len(rg_files),rg_files)\n",
    "    all_sessions = []\n",
    "    for file, text_file in zip(rg_files,text_files):\n",
    "        print(\"file: \",file)\n",
    "        with open(text_file) as f:\n",
    "            lines = f.readlines()\n",
    "       # print(\"medication state: \",lines[5],lines[4])\n",
    "        df = pd.read_csv(file)\n",
    "        df =df[df[\"activity\"]==1]\n",
    "#         df_peaks_valleys, peak_indexs, valley_idxs,peak_points = peaks_and_valleys(df)\n",
    "#         print(\"file.split [12]: \",file.split(\"/\")[11])\n",
    "#         #df.to_csv(dset_path +'/Participant'+str(pno) + \"/\"+ file.split(\"/\")[12] + \"/\" + file.split(\"/\")[13] + \"_peak_results.csv\",index=0)\n",
    "#         rise_time_arr  = rise_time(peak_indexs,valley_idxs)\n",
    "#         print(\"peak_points,rise_time_arr: \",len(peak_points),len(rise_time_arr))\n",
    "#         # print(\"rise_time_arr: \",rise_time_arr)\n",
    "#        # print(\"rise_time_arr: \", rise_time_arr)\n",
    "#         # create a list of medication statius same length as rise_time array.\n",
    "#         med_status = [lines[4]] * len(peak_points) \n",
    "#         # gets the dates of the participants. \n",
    "#         date_list = [file.split(\"/\")[11]] * len(peak_points)\n",
    "#         file_id = [file.split(\"/\")[12]] *len(peak_points)\n",
    "#         session1 = pd.DataFrame(\n",
    "#             {#'rise_time_arr_seconds': rise_time_arr,\n",
    "#             'peak_amplitude': peak_points,\n",
    "#              'med_status': med_status,\n",
    "#             'date_list': date_list,\n",
    "#             'file_id_session_num': file_id\n",
    "#             })\n",
    "#         all_sessions.append(session1)\n",
    "#     all_sessions_df = pd.concat(all_sessions)\n",
    "# all_sessions_df.to_csv(\"/Users/shehjarsadhu/Desktop/UniversityOfRhodeIsland/Graduate/WBL/Project_IOTEX/iotex-glove/PD/Participant1/peak_amplitudes.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a6f65-0826-4b98-a074-341f2312cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files ending with a .txt.\n",
    "all_files_txt = glob.glob(\"/Users/shehjarsadhu/Desktop/UniversityOfRhodeIsland/Graduate/WBL/Project_IOTEX/iotex-glove/PD/*\")\n",
    "\n",
    "#print(\"all_files_txt = \",all_files_txt,len(all_files_txt))\n",
    "participant_list = []\n",
    "date_list = []\n",
    "num_task = []\n",
    "file_paths_lg = []\n",
    "counter_lg = 0\n",
    "file_paths_rg = []\n",
    "file_paths_txt = []\n",
    "participant_list_lg = []\n",
    "date_list_lg_all = []\n",
    "file_name_lg_all = []\n",
    "participant_list_rg = []\n",
    "date_list_rg_all = []\n",
    "file_name_rg_all = []\n",
    "participant_list_txt = []\n",
    "date_list_txt_all = []\n",
    "file_name_txt_all = []\n",
    "# Read Files.\n",
    "for full_path in all_files_txt:\n",
    "    #print(\"ROOT FOLDER PATH \",full_path)\n",
    "    all_folder_dates =  glob.glob(full_path + \"/*\")\n",
    "    #print(all_folder_dates) \n",
    "    # Get a list of dates based on Participant ID\n",
    "    for dates_folder in all_folder_dates:\n",
    "        d_split = dates_folder.split(\"/\")\n",
    "        #print(\"d_split == \\n\",d_split[9],d_split[10])\n",
    "        participant_list.append(d_split[9])\n",
    "        date_list.append(d_split[10])\n",
    "        #list all .csvfiles in that dates folder.\n",
    "        dates_folder_files = glob.glob(dates_folder + \"/*\")\n",
    "        for i in dates_folder_files:\n",
    "            #print(i.split(\"/\"))\n",
    "            file_name = i.split(\"/\")[12]\n",
    "            if file_name.startswith(\"lg\"):\n",
    "                print(\"LG\",i.split(\"/\")[9],i.split(\"/\")[10] , file_name)\n",
    "                counter_lg+=1\n",
    "                print(\"counter_lg = \\n\",counter_lg)\n",
    "                print(i)\n",
    "                participant_list_lg.append(i.split(\"/\")[10])\n",
    "                date_list_lg_all.append(i.split(\"/\")[11])\n",
    "                file_name_lg_all.append(i.split(\"/\")[12])\n",
    "                #num_task.append(counter_lg)\n",
    "                file_paths_lg.append(i)\n",
    "            if file_name.startswith(\"rg\"):\n",
    "                participant_list_rg.append(i.split(\"/\")[10])\n",
    "                date_list_rg_all.append(i.split(\"/\")[11])\n",
    "                file_paths_rg.append(i.split(\"/\")[12])\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                participant_list_txt.append(i.split(\"/\")[10])\n",
    "                date_list_txt_all.append(i.split(\"/\")[11])\n",
    "                file_paths_txt.append(i.split(\"/\")[12])\n",
    "        num_task.append(counter_lg)\n",
    "        counter_lg = 0 # Reset counting fils after each date is visited.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b1061-8b76-4d27-a41d-08799f86d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(participant_list_lg),len(date_list_lg_all),len(file_name_lg_all))\n",
    "print(len(participant_list_rg),len(date_list_rg_all),len(file_paths_rg))\n",
    "print(len(participant_list_txt),len(date_list_txt_all),len(file_paths_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3facaa-57ff-4a4f-b198-9b374fe27431",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_paths_lg = pd.DataFrame(\n",
    "    {'ParticipantList': participant_list_lg,\n",
    "     'DateList': date_list_lg_all,\n",
    "     \"FileName\":file_name_lg_all \n",
    "    })\n",
    "df_file_paths_rg = pd.DataFrame(\n",
    "    {'ParticipantList': participant_list_rg,\n",
    "     'DateList': date_list_rg_all,\n",
    "     \"FileName\":file_paths_rg \n",
    "    })\n",
    "df_file_paths_txt = pd.DataFrame(\n",
    "    {'ParticipantList': participant_list_txt,\n",
    "     'DateList': date_list_txt_all,\n",
    "     \"FileName\":file_paths_txt \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e44742-1917-45ba-b1dd-7eab43541c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_list(df_file_paths_rg,df_file_paths_txt,pid):\n",
    "    rg_fp = []\n",
    "    meds_fp = []\n",
    "    new_date_list = []\n",
    "    participant_id =pid \n",
    "    part_uniq_day = df_file_paths_txt[df_file_paths_txt[\"ParticipantList\"]==participant_id][\"DateList\"].unique()\n",
    "    for idx, day in enumerate(part_uniq_day):\n",
    "        df_file_paths_rg_subset_day = df_file_paths_rg[df_file_paths_rg[\"ParticipantList\"]==participant_id]\n",
    "        df_file_paths_txt_subset_day = df_file_paths_txt[df_file_paths_txt[\"ParticipantList\"]==participant_id]\n",
    "        print(\"Unique Day :================\",df_file_paths_rg[\"DateList\"].unique()[idx],\"================\")\n",
    "        df_file_paths_rg_subset_day =  df_file_paths_rg_subset_day[ df_file_paths_rg_subset_day[\"DateList\"]== df_file_paths_rg_subset_day[\"DateList\"].unique()[idx]]\n",
    "        df_file_paths_txt_subset_day = df_file_paths_txt_subset_day[df_file_paths_txt_subset_day[\"DateList\"]==df_file_paths_txt_subset_day[\"DateList\"].unique()[idx]]\n",
    "        print(df_file_paths_txt_subset_day)\n",
    "        for index, row in df_file_paths_txt_subset_day.iterrows():\n",
    "            str_filetime_info = df_file_paths_txt.iloc[index][\"FileName\"].split(\"-\")[1].split(\"_\")[0]\n",
    "            for index2, row2 in df_file_paths_rg_subset_day.iterrows():\n",
    "                str_filetime_rg = df_file_paths_rg.iloc[index2][\"FileName\"].split(\"-\")[1].split(\".\")[0]\n",
    "                #print(\"int(str_filetime_info=\",int(str_filetime_info[:2]))\n",
    "                #print(\"int(str_filetime_rg=\",int(str_filetime_rg[:2]))\n",
    "                if int(str_filetime_rg[:2]) == int(str_filetime_info[:2]):\n",
    "                    print(\"Matching pairs for Meds\",(str_filetime_rg ,str_filetime_info))\n",
    "                    print(\"Matching File Paths for Meds\",(df_file_paths_rg.iloc[index2][\"FileName\"],df_file_paths_txt.iloc[index][\"FileName\"].split(\"-\")[1]))\n",
    "                    rg_fp.append(df_file_paths_rg.iloc[index2][\"FileName\"])\n",
    "                    meds_fp.append(df_file_paths_txt.iloc[index][\"FileName\"].split(\"-\")[1])\n",
    "                    new_date_list.append(df_file_paths_rg.iloc[index2][\"DateList\"])\n",
    "    return rg_fp, meds_fp, new_date_list\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe256dea-76e2-47e5-afa9-1d11b296867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_fp, meds_fp, new_date_list = file_list(df_file_paths_rg,df_file_paths_txt,\"Participant4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d48876-9cb8-4b51-9f62-4a9903e208a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_file_paths_rg_info_sync = pd.DataFrame({\n",
    "     'DateList': new_date_list,\n",
    "     \"FileName\":rg_fp ,\n",
    "    \"FileName_InfoGlove\":meds_fp })\n",
    "\n",
    "df_file_paths_lg_info_sync = pd.DataFrame({\n",
    "     'DateList': new_lg_date_list,\n",
    "     \"FileName\":lg_fp ,\n",
    "    \"FileName_InfoGlove\":meds_fp2 })\n",
    "\n",
    "result = pd.concat([df_file_paths_rg_info_sync,df_file_paths_lg_info_sync])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb6c44-7a7a-42ba-a2ec-82714f595576",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"/Users/shehjarsadhu/Desktop/UniversityOfRhodeIsland/Graduate/WBL/Project_IOTEX/iotex-glove/info_sync_filepaths_p4.csv\",index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbccd37b-347d-4eec-ae97-fd18ff5db153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
