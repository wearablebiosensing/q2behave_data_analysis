{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9722605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the file I used to run blobs of segments files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1729b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sc\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.integrate import simps\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0a464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gender_age(df,filename):\n",
    "    if 'P5' in filename:\n",
    "        df['gender'] = 'M'\n",
    "        df['age'] = 10\n",
    "    if 'P6' in filename:\n",
    "        df['gender'] = 'M'\n",
    "        df['age'] = 8\n",
    "    if 'P7' in filename:\n",
    "        df['gender'] = 'M'\n",
    "        df['age'] = 7\n",
    "    if 'P8' in filename:\n",
    "        df['gender'] = 'F'\n",
    "        df['age'] = 6\n",
    "        \n",
    "    if 'P9' in filename:\n",
    "        df['gender'] = 'F'\n",
    "        df['age'] = 7\n",
    "    if 'P10' in filename:\n",
    "        df['gender'] = 'F'\n",
    "        df['age'] = 10\n",
    "    if 'P11' in filename:\n",
    "        df['gender'] = 'F'\n",
    "        df['age'] = 7\n",
    "    if 'P12' in filename:\n",
    "        df['gender'] = 'M'\n",
    "        df['age'] = 11\n",
    "        \n",
    "    if 'P13' in filename:\n",
    "        df['gender'] = 'M'\n",
    "        df['age'] = 8\n",
    "        \n",
    "    if 'P14' in filename:\n",
    "        df['gender'] = 'M'\n",
    "        df['age'] = 10\n",
    "    if 'P15' in filename:\n",
    "        df['gender'] = 'M'\n",
    "        df['age'] = 8\n",
    "        \n",
    "    if 'P16' in filename:\n",
    "        df['gender'] = 'F'\n",
    "        df['age'] = 7\n",
    "    if 'P17' in filename:\n",
    "        df['gender'] = 'M'\n",
    "        df['age'] = 9\n",
    "    if 'P18' in filename:\n",
    "        df['gender'] = 'M'\n",
    "        df['age'] = 6\n",
    "        \n",
    "    if 'P19' in filename:\n",
    "        df['gender'] = 'M'\n",
    "        df['age'] = 11\n",
    "    if 'P20' in filename:\n",
    "        df['gender'] = 'F'\n",
    "        df['age'] = 11\n",
    "    if 'P21' in filename:\n",
    "        df['gender'] = 'F'\n",
    "        df['age'] = 10\n",
    "        \n",
    "    if 'P22' in filename:\n",
    "        df['gender'] = 'F'\n",
    "        df['age'] = 7\n",
    "    if 'P23' in filename:\n",
    "        df['gender'] = 'F'\n",
    "        df['age'] = 7\n",
    "    if 'P24' in filename:\n",
    "        df['gender'] = 'F'\n",
    "        df['age'] = 7\n",
    "    return df\n",
    "\n",
    "def add_specific_hyperactive_behavior(df,filename):\n",
    "    file_components = filename.split('_')\n",
    "    if 'NoHyp' in file_components:\n",
    "        df['specific_hyp'] = 0\n",
    "    if 'peers' in file_components:\n",
    "        df['specific_hyp'] = 1\n",
    "    if 'self' in file_components:\n",
    "        df['specific_hyp'] = 2\n",
    "    if 'Twirling' in file_components:\n",
    "        df['specific_hyp'] = 3\n",
    "    if 'chair' in file_components:\n",
    "        df['specific_hyp'] = 4\n",
    "    if 'Manipulating' in file_components:\n",
    "        df['specific_hyp'] = 5\n",
    "    if 'Standing' in file_components:\n",
    "        df['specific_hyp'] = 6\n",
    "    if 'Drumming_fingers' in filename:\n",
    "        df['specific_hyp'] = 7\n",
    "    if 'Bouncing_your_leg' in filename:\n",
    "        df['specific_hyp'] = 8\n",
    "    if 'Finger_tapping' in filename:\n",
    "        df['specific_hyp'] = 9\n",
    "    if 'Tapping_foot' in filename:\n",
    "        df['specific_hyp'] = 10\n",
    "    if 'Shaking_leg' in filename:\n",
    "        df['specific_hyp'] = 11\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43bfd3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(X, window_size):\n",
    "    X_new =[]\n",
    "    # end_index = window_size\n",
    "    for i, value in enumerate(X):\n",
    "        #print(\"index, value\",i, value)\n",
    "        X_new.insert(i, sum(X[i:i+window_size])/len(X[i:i+window_size]))\n",
    "    return X_new\n",
    "\n",
    "def moving_average_dataframe(X, window_size):\n",
    "    X_new = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "    for column in X.columns:\n",
    "        X_new[column] = X[column].rolling(window=window_size).mean()\n",
    "    X_new = X_new.dropna()\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd0f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sampling_frequency(dataframe):\n",
    "    frequencies = dataframe['timestamp_str_seconds'].value_counts()\n",
    "    average_frequency = sum(frequencies) / len(frequencies)\n",
    "\n",
    "    return average_frequency\n",
    "\n",
    "\n",
    "def calculate_fft(data, sampling_rate):\n",
    "    data_no_dc = data - data.mean()\n",
    "    fft_result = np.fft.fft(data_no_dc, axis=0)\n",
    "    #freqs = np.fft.fftfreq(len(fft_result), 1/sampling_rate)\n",
    "    return pd.DataFrame(abs(fft_result),  columns=data.columns)\n",
    "\n",
    "def dominant_frequency(data, sampling_rate, feature_name):\n",
    "    data_no_dc = data - data.mean()\n",
    "    fft_result = np.fft.fft(data_no_dc, axis=0)\n",
    "    freqs = np.fft.fftfreq(len(fft_result), 1/sampling_rate)\n",
    "    dominant_freq = np.abs(freqs[np.argmax(np.abs(fft_result), axis=0)])\n",
    "    dominant_df = pd.DataFrame(dominant_freq).transpose()\n",
    "    dominant_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name, \"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name]\n",
    "    return dominant_df\n",
    "\n",
    "\n",
    "def maximum_amplitude(data, sampling_rate, feature_name):\n",
    "    data_no_dc = data - data.mean()\n",
    "    fft_result = np.fft.fft(data_no_dc, axis=0)\n",
    "    #freqs = np.fft.fftfreq(len(fft_result), 1/sampling_rate)\n",
    "    # Calculate maximum amplitude for each column\n",
    "    max_amplitude = np.max(np.abs(fft_result), axis=0)\n",
    "    amplitude_df = pd.DataFrame(max_amplitude).transpose()\n",
    "    amplitude_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name, \"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name]\n",
    "    return amplitude_df\n",
    "\n",
    "\n",
    "def calculate_q_factor(data, sampling_rate, feature_name):\n",
    "    # Find peaks in the data\n",
    "    q_factors = []\n",
    "\n",
    "    for column_name in data.columns:\n",
    "        column_data = data[column_name].values  # Extract column as a NumPy array\n",
    "\n",
    "        # Find peaks in the data\n",
    "        peaks, _ = find_peaks(column_data)\n",
    "\n",
    "        if len(peaks) < 1:\n",
    "            q_factors.append(None)  # No peaks found for this column\n",
    "            continue\n",
    "\n",
    "        # Calculate peak width\n",
    "        widths, _, _, _ = peak_widths(column_data, peaks)\n",
    "\n",
    "        if len(widths) < 1:\n",
    "            q_factors.append(None)  # Unable to calculate Q factor for this column\n",
    "            continue\n",
    "\n",
    "        # Calculate Q factor using the formula: f_peak / width\n",
    "        max_index = np.argmax(column_data)\n",
    "        peak_frequency = peaks[np.argmin(np.abs(peaks - max_index))]  # Find the nearest peak\n",
    "        q_factor = peak_frequency / widths[0]\n",
    "\n",
    "        # Append the calculated Q factor to the list\n",
    "        q_factors.append(q_factor)\n",
    "        \n",
    "    q_factor_df = pd.DataFrame(q_factors).transpose()\n",
    "    q_factor_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name, \"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name]\n",
    "    return q_factor_df\n",
    "\n",
    "\n",
    "def calculate_auc_feature(feature_name, accel_gyro_data):\n",
    "    freq = accel_gyro_data.index\n",
    "    \n",
    "    # Use NumPy vectorized operations to calculate AUC for each column\n",
    "    auc_values = np.trapz(accel_gyro_data.values, freq, axis=0)\n",
    "    \n",
    "    columns = [f\"{col}_{feature_name}\" for col in accel_gyro_data.columns]\n",
    "    #print(auc_values)\n",
    "    auc_df = pd.DataFrame(auc_values).T\n",
    "    auc_df.columns = columns\n",
    "    \n",
    "    return auc_df\n",
    "\n",
    "\n",
    "\n",
    "# features to analyse\n",
    "#mean\n",
    "def feature_mean(feature_name,accel_gyro_data):\n",
    "    mean = accel_gyro_data.mean()\n",
    "    mean_transposed_df = mean.to_frame().T\n",
    "    mean_transposed_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name ,\"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name, \"Accel_Vecdis_\" +feature_name, \"Gyro_Vecdis_\" +feature_name ,\"Magno_Vecdis_\" +feature_name]\n",
    "    return mean_transposed_df\n",
    "\n",
    "#std\n",
    "def feature_std(feature_name,accel_gyro_data):\n",
    "    std = accel_gyro_data.std()\n",
    "    std_transposed_df = std.to_frame().T\n",
    "    std_transposed_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name ,\"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name, \"Accel_Vecdis_\" +feature_name, \"Gyro_Vecdis_\" +feature_name ,\"Magno_Vecdis_\" +feature_name]\n",
    "    return std_transposed_df\n",
    "\n",
    "#kurtosis\n",
    "def feature_kurtosis(feature_name,accel_gyro_data):\n",
    "    kurtosis = accel_gyro_data.kurtosis()\n",
    "    kurtosis_transposed_df = kurtosis.to_frame().T\n",
    "    kurtosis_transposed_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name ,\"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name, \"Accel_Vecdis_\" +feature_name, \"Gyro_Vecdis_\" +feature_name ,\"Magno_Vecdis_\" +feature_name]\n",
    "    return kurtosis_transposed_df\n",
    "\n",
    "#median\n",
    "def feature_median(feature_name,accel_gyro_data):\n",
    "    median = accel_gyro_data.median()\n",
    "    median_transposed_df = median.to_frame().T\n",
    "    median_transposed_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name ,\"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name, \"Accel_Vecdis_\" +feature_name, \"Gyro_Vecdis_\" +feature_name ,\"Magno_Vecdis_\" +feature_name]\n",
    "    return median_transposed_df\n",
    "\n",
    "#skewness\n",
    "def feature_skewness(feature_name,accel_gyro_data):\n",
    "    skewness = accel_gyro_data.skew()\n",
    "    skewness_transposed_df = skewness.to_frame().T\n",
    "    skewness_transposed_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name ,\"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name, \"Accel_Vecdis_\" +feature_name, \"Gyro_Vecdis_\" +feature_name ,\"Magno_Vecdis_\" +feature_name]\n",
    "    return skewness_transposed_df\n",
    "\n",
    "# vector distance mean\n",
    "def feature_sum(feature_name,accel_gyro_data):\n",
    "    sum_ = accel_gyro_data.sum()\n",
    "    sum_transposed_df = sum_.to_frame().T\n",
    "    sum_transposed_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name ,\"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name, \"Accel_Vecdis_\" +feature_name, \"Gyro_Vecdis_\" +feature_name ,\"Magno_Vecdis_\" +feature_name]\n",
    "    return sum_transposed_df\n",
    "\n",
    "def calculate_zero_crossing_rate(accel_data):\n",
    "    accel_data = (accel_data - np.mean(accel_data)) / np.std(accel_data)\n",
    "        # Calculate Zerocrossing rate.\n",
    "    zero_crossings = np.nonzero(np.diff(np.signbit(accel_data)))[0]\n",
    "    zcr = len(zero_crossings) / (2.0 * len(accel_data))\n",
    "    return zcr\n",
    "\n",
    "def feature_zero_cross(feature_name,accel_gyro_data):\n",
    "    zero_crossing_rates = {}\n",
    "    for column in accel_gyro_data.columns:\n",
    "        zcr = calculate_zero_crossing_rate(accel_gyro_data[column])\n",
    "        zero_crossing_rates[column] = zcr\n",
    "\n",
    "    zcr_series = pd.Series(zero_crossing_rates)\n",
    "    zcr_transposed_df = zcr_series.to_frame().T\n",
    "    zcr_transposed_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name ,\"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name, \"Accel_Vecdis_\" +feature_name, \"Gyro_Vecdis_\" +feature_name ,\"Magno_Vecdis_\" +feature_name]\n",
    "    return zcr_transposed_df\n",
    "\n",
    "\n",
    "# vector distance mean\n",
    "def diff_sum(feature_name,accel_gyro_data):\n",
    "    sum_ = accel_gyro_data.diff().sum()\n",
    "    sum_transposed_df = sum_.to_frame().T\n",
    "    sum_transposed_df.columns = [\"Accel_X_\"+feature_name ,\"Accel_Y_\" +feature_name ,\"Accel_Z_\" +feature_name,\"Gyro_X_\" +feature_name, \"Gyro_Y_\" + feature_name ,\"Gyro_Z_\" +feature_name ,\"Magno_X_\" +feature_name, \"Magno_Y_\" +feature_name ,\"Magno_Z_\" +feature_name, \"Accel_Vecdis_\" +feature_name, \"Gyro_Vecdis_\" +feature_name ,\"Magno_Vecdis_\" +feature_name]\n",
    "    return sum_transposed_df\n",
    "\n",
    "   \n",
    "# create table \n",
    "def get_concated_features_2(not_normalized_accel_gyro_data,accel_gyro_data,sampling_rate):\n",
    "    \n",
    "    ma_not_normalized = moving_average_dataframe(not_normalized_accel_gyro_data, 5)\n",
    "    ma_normalized_data = scaler.fit_transform(ma_not_normalized)\n",
    "    ma_normalized = pd.DataFrame(ma_normalized_data, columns=ma_not_normalized.columns)\n",
    "    \n",
    "    df9 = dominant_frequency(ma_normalized, sampling_rate, 'dominant_freq')\n",
    "    df10 = maximum_amplitude(ma_normalized, sampling_rate, 'max_amplitude')\n",
    "    #df11 = calculate_q_factor(not_normalized_accel_gyro_data, sampling_rate, 'q_factor')\n",
    "    \n",
    "\n",
    "    \n",
    "    accel_gyro_data['Accel'] = np.sqrt(np.sum(accel_gyro_data.iloc[:, 0:3] ** 2, axis = 1))\n",
    "    accel_gyro_data['Gyro'] = np.sqrt(np.sum(accel_gyro_data.iloc[:, 3:6] ** 2, axis = 1))\n",
    "    accel_gyro_data['Magno'] = np.sqrt(np.sum(accel_gyro_data.iloc[:, 6:9] ** 2, axis = 1))\n",
    "    \n",
    "    not_normalized_accel_gyro_data['Accel'] = np.sqrt(np.sum(not_normalized_accel_gyro_data.iloc[:, 0:3] ** 2, axis = 1))\n",
    "    not_normalized_accel_gyro_data['Gyro'] = np.sqrt(np.sum(not_normalized_accel_gyro_data.iloc[:, 3:6] ** 2, axis = 1))\n",
    "    not_normalized_accel_gyro_data['Magno'] = np.sqrt(np.sum(not_normalized_accel_gyro_data.iloc[:, 6:9] ** 2, axis = 1))\n",
    "    \n",
    "    ma_not_normalized_2 = moving_average_dataframe(not_normalized_accel_gyro_data, 5)\n",
    "    ma_normalized_data_2 = scaler.fit_transform(ma_not_normalized_2)\n",
    "    ma_normalized_2 = pd.DataFrame(ma_normalized_data_2, columns=ma_not_normalized_2.columns)\n",
    "    \n",
    "    \n",
    "    fft = calculate_fft(ma_normalized_2, sampling_rate)\n",
    "    \n",
    "    df12 = feature_mean(\"mean_fft\",fft)\n",
    "    df13 = feature_median(\"median_fft\",fft)\n",
    "    df14 = feature_std(\"std_fft\",fft)\n",
    "    df15 = feature_kurtosis(\"kurtosis_fft\",fft)\n",
    "    df16 =feature_skewness(\"skewness_fft\",fft)\n",
    "    df17 = feature_sum(\"sum_fft\",fft)\n",
    "    df18 = feature_zero_cross(\"zero_cross_rate_fft\", fft)\n",
    "    df19 = diff_sum(\"diff_sum_fft\", fft)\n",
    "    df20 = calculate_auc_feature('AUC_fft', fft)\n",
    "    \n",
    "    df1 = feature_mean(\"mean\",accel_gyro_data)\n",
    "    df2 = feature_median(\"median\",accel_gyro_data)\n",
    "    df3 = feature_std(\"std\",accel_gyro_data)\n",
    "    df4 = feature_kurtosis(\"kurtosis\",accel_gyro_data)\n",
    "    df5 =feature_skewness(\"skewness\",accel_gyro_data)\n",
    "    df6 = feature_sum(\"sum\",accel_gyro_data)\n",
    "    df7 = feature_zero_cross(\"zero_cross_rate\", not_normalized_accel_gyro_data)\n",
    "    df8 = diff_sum(\"diff_sum\", accel_gyro_data)\n",
    "\n",
    "    df = [df1, df2, df3, df4, df5, df6,df7, df8,df9, df10,df12,df13,df14,df15,df16,df17,df18,df19,df20]\n",
    "    result_df = pd.concat(df, axis=1)\n",
    "\n",
    "    return result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369e66df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MinMaxScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mMinMaxScaler\u001b[49m()\n\u001b[0;32m      2\u001b[0m directory_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mfried\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mURI\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mELE_591\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124msyncedfiles\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mBlobsOFSegments\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create an empty list to store the DataFrames\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MinMaxScaler' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "directory_path = 'C:\\\\Users\\\\fried\\\\Documents\\\\URI\\\\ELE_591\\\\syncedfiles\\\\BlobsOFSegments'\n",
    "\n",
    "# Create an empty list to store the DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame and append it to the list\n",
    "        df = pd.read_csv(file_path,low_memory=False)\n",
    "        print(filename)\n",
    "        \n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df = df.drop(columns='Unnamed: 0')\n",
    "        \n",
    "\n",
    "        df_not_normalized = df.iloc[:, 4:13]\n",
    "        #df_normalized = pd.DataFrame(scaler.fit_transform(df.iloc[:, 4:13]), columns=df.columns)\n",
    "        df_normalized = pd.DataFrame(scaler.fit_transform(df.iloc[:, 4:13]), columns=df.iloc[:, 4:13].columns)\n",
    "        sampling_rate  = calculate_sampling_frequency(df)\n",
    "        hyper_result = get_concated_features_2(df_not_normalized,df_normalized ,sampling_rate)\n",
    "\n",
    "        resulting_df = hyper_result#pd.concat(hyper_result, axis=0)\n",
    "\n",
    "        if 'NoHyp' in filename:\n",
    "            resulting_df['type'] = 0\n",
    "        else:\n",
    "            resulting_df['type'] = 1\n",
    "\n",
    "        # split file name and add columns\n",
    "        file_components = filename.split('_')\n",
    "        resulting_df['Participent'] = file_components[0]\n",
    "        resulting_df['Watch ID'] = file_components[1]\n",
    "        resulting_df['Activity'] = file_components[2]\n",
    "        \n",
    "        resulting_df_new_1 = add_gender_age(resulting_df,filename)\n",
    "        resulting_df_new = add_specific_hyperactive_behavior(resulting_df_new_1,filename)\n",
    "        behavior_string = ' '.join(file_components[3:])\n",
    "        resulting_df_new['Behavior'] = behavior_string\n",
    "        \n",
    "        dataframes.append(resulting_df_new)\n",
    "        \n",
    "        \n",
    "\n",
    "resulting_df_final = pd.concat(dataframes, axis=0)\n",
    "resulting_df_final.head()\n",
    "#print(dataframes[0])  \n",
    "resulting_df_final.to_csv('C:\\\\Users\\\\fried\\\\Documents\\\\URI\\\\ELE_591\\\\syncedfiles\\\\analysis_table_blobs_of_segments_with_mov_average_freq.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3146b5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accel_X</th>\n",
       "      <th>Accel_Y</th>\n",
       "      <th>Accel_Z</th>\n",
       "      <th>Gyro_X</th>\n",
       "      <th>Gyro_Y</th>\n",
       "      <th>Gyro_Z</th>\n",
       "      <th>Magno_X</th>\n",
       "      <th>Magno_Y</th>\n",
       "      <th>Magno_Z</th>\n",
       "      <th>Accel</th>\n",
       "      <th>Gyro</th>\n",
       "      <th>Magno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.593413</td>\n",
       "      <td>0.550690</td>\n",
       "      <td>0.740733</td>\n",
       "      <td>0.656350</td>\n",
       "      <td>0.671607</td>\n",
       "      <td>0.715695</td>\n",
       "      <td>0.436727</td>\n",
       "      <td>0.302746</td>\n",
       "      <td>0.106967</td>\n",
       "      <td>0.505830</td>\n",
       "      <td>0.127891</td>\n",
       "      <td>0.334348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.582635</td>\n",
       "      <td>0.566075</td>\n",
       "      <td>0.757582</td>\n",
       "      <td>0.690486</td>\n",
       "      <td>0.679179</td>\n",
       "      <td>0.691433</td>\n",
       "      <td>0.436251</td>\n",
       "      <td>0.284330</td>\n",
       "      <td>0.142153</td>\n",
       "      <td>0.522639</td>\n",
       "      <td>0.128037</td>\n",
       "      <td>0.190713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.566267</td>\n",
       "      <td>0.598028</td>\n",
       "      <td>0.777602</td>\n",
       "      <td>0.706576</td>\n",
       "      <td>0.685467</td>\n",
       "      <td>0.661319</td>\n",
       "      <td>0.426736</td>\n",
       "      <td>0.260420</td>\n",
       "      <td>0.179451</td>\n",
       "      <td>0.530832</td>\n",
       "      <td>0.111665</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.561876</td>\n",
       "      <td>0.660355</td>\n",
       "      <td>0.794450</td>\n",
       "      <td>0.743393</td>\n",
       "      <td>0.704588</td>\n",
       "      <td>0.611217</td>\n",
       "      <td>0.424833</td>\n",
       "      <td>0.235218</td>\n",
       "      <td>0.164673</td>\n",
       "      <td>0.501220</td>\n",
       "      <td>0.115441</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.553892</td>\n",
       "      <td>0.764892</td>\n",
       "      <td>0.791278</td>\n",
       "      <td>0.781664</td>\n",
       "      <td>0.733397</td>\n",
       "      <td>0.544020</td>\n",
       "      <td>0.429115</td>\n",
       "      <td>0.202262</td>\n",
       "      <td>0.128079</td>\n",
       "      <td>0.389446</td>\n",
       "      <td>0.145058</td>\n",
       "      <td>0.209829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.545908</td>\n",
       "      <td>0.745957</td>\n",
       "      <td>0.779187</td>\n",
       "      <td>0.813230</td>\n",
       "      <td>0.753032</td>\n",
       "      <td>0.470840</td>\n",
       "      <td>0.439106</td>\n",
       "      <td>0.176414</td>\n",
       "      <td>0.101337</td>\n",
       "      <td>0.391155</td>\n",
       "      <td>0.185615</td>\n",
       "      <td>0.366950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.541317</td>\n",
       "      <td>0.879684</td>\n",
       "      <td>0.788107</td>\n",
       "      <td>0.938041</td>\n",
       "      <td>0.785627</td>\n",
       "      <td>0.389835</td>\n",
       "      <td>0.453378</td>\n",
       "      <td>0.141519</td>\n",
       "      <td>0.066854</td>\n",
       "      <td>0.299389</td>\n",
       "      <td>0.320052</td>\n",
       "      <td>0.588005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.566467</td>\n",
       "      <td>0.949507</td>\n",
       "      <td>0.765709</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.793391</td>\n",
       "      <td>0.325728</td>\n",
       "      <td>0.480495</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.055595</td>\n",
       "      <td>0.179955</td>\n",
       "      <td>0.401276</td>\n",
       "      <td>0.801501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.661876</td>\n",
       "      <td>0.667061</td>\n",
       "      <td>0.873538</td>\n",
       "      <td>0.993240</td>\n",
       "      <td>0.815464</td>\n",
       "      <td>0.293445</td>\n",
       "      <td>0.506660</td>\n",
       "      <td>0.056220</td>\n",
       "      <td>0.070373</td>\n",
       "      <td>0.703714</td>\n",
       "      <td>0.422030</td>\n",
       "      <td>0.915206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.666866</td>\n",
       "      <td>0.801578</td>\n",
       "      <td>0.921506</td>\n",
       "      <td>0.953908</td>\n",
       "      <td>0.878537</td>\n",
       "      <td>0.312841</td>\n",
       "      <td>0.545195</td>\n",
       "      <td>0.023263</td>\n",
       "      <td>0.107671</td>\n",
       "      <td>0.732823</td>\n",
       "      <td>0.436930</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.691417</td>\n",
       "      <td>0.901775</td>\n",
       "      <td>0.946283</td>\n",
       "      <td>0.869658</td>\n",
       "      <td>0.912672</td>\n",
       "      <td>0.330265</td>\n",
       "      <td>0.586584</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.178747</td>\n",
       "      <td>0.676972</td>\n",
       "      <td>0.414252</td>\n",
       "      <td>0.969607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.693613</td>\n",
       "      <td>0.763708</td>\n",
       "      <td>0.961943</td>\n",
       "      <td>0.669982</td>\n",
       "      <td>0.933462</td>\n",
       "      <td>0.359590</td>\n",
       "      <td>0.635585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258269</td>\n",
       "      <td>0.824384</td>\n",
       "      <td>0.320156</td>\n",
       "      <td>0.931425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.644511</td>\n",
       "      <td>0.734122</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.580088</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.391150</td>\n",
       "      <td>0.680304</td>\n",
       "      <td>0.018740</td>\n",
       "      <td>0.325827</td>\n",
       "      <td>0.953320</td>\n",
       "      <td>0.290748</td>\n",
       "      <td>0.894140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.526946</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909415</td>\n",
       "      <td>0.532097</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.402196</td>\n",
       "      <td>0.731684</td>\n",
       "      <td>0.042326</td>\n",
       "      <td>0.396200</td>\n",
       "      <td>0.490295</td>\n",
       "      <td>0.295560</td>\n",
       "      <td>0.883231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.477844</td>\n",
       "      <td>0.900197</td>\n",
       "      <td>0.903865</td>\n",
       "      <td>0.476228</td>\n",
       "      <td>0.962207</td>\n",
       "      <td>0.382011</td>\n",
       "      <td>0.768316</td>\n",
       "      <td>0.078191</td>\n",
       "      <td>0.492611</td>\n",
       "      <td>0.560222</td>\n",
       "      <td>0.290083</td>\n",
       "      <td>0.725974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.422954</td>\n",
       "      <td>0.825247</td>\n",
       "      <td>0.896729</td>\n",
       "      <td>0.422258</td>\n",
       "      <td>0.940776</td>\n",
       "      <td>0.387402</td>\n",
       "      <td>0.800190</td>\n",
       "      <td>0.109855</td>\n",
       "      <td>0.553132</td>\n",
       "      <td>0.639696</td>\n",
       "      <td>0.311387</td>\n",
       "      <td>0.678097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.412575</td>\n",
       "      <td>0.814201</td>\n",
       "      <td>0.873538</td>\n",
       "      <td>0.436337</td>\n",
       "      <td>0.913699</td>\n",
       "      <td>0.414952</td>\n",
       "      <td>0.825880</td>\n",
       "      <td>0.142488</td>\n",
       "      <td>0.601689</td>\n",
       "      <td>0.606253</td>\n",
       "      <td>0.278673</td>\n",
       "      <td>0.646768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.436926</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.868385</td>\n",
       "      <td>0.414939</td>\n",
       "      <td>0.876805</td>\n",
       "      <td>0.442107</td>\n",
       "      <td>0.849667</td>\n",
       "      <td>0.176090</td>\n",
       "      <td>0.656580</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.263885</td>\n",
       "      <td>0.597178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.452096</td>\n",
       "      <td>0.785799</td>\n",
       "      <td>0.821407</td>\n",
       "      <td>0.377116</td>\n",
       "      <td>0.848252</td>\n",
       "      <td>0.485897</td>\n",
       "      <td>0.872027</td>\n",
       "      <td>0.211632</td>\n",
       "      <td>0.693174</td>\n",
       "      <td>0.504112</td>\n",
       "      <td>0.269774</td>\n",
       "      <td>0.602620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.462475</td>\n",
       "      <td>0.530572</td>\n",
       "      <td>0.788305</td>\n",
       "      <td>0.273311</td>\n",
       "      <td>0.852486</td>\n",
       "      <td>0.576698</td>\n",
       "      <td>0.902950</td>\n",
       "      <td>0.242973</td>\n",
       "      <td>0.706545</td>\n",
       "      <td>0.679756</td>\n",
       "      <td>0.363115</td>\n",
       "      <td>0.750508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accel_X   Accel_Y   Accel_Z    Gyro_X    Gyro_Y    Gyro_Z   Magno_X  \\\n",
       "0   0.593413  0.550690  0.740733  0.656350  0.671607  0.715695  0.436727   \n",
       "1   0.582635  0.566075  0.757582  0.690486  0.679179  0.691433  0.436251   \n",
       "2   0.566267  0.598028  0.777602  0.706576  0.685467  0.661319  0.426736   \n",
       "3   0.561876  0.660355  0.794450  0.743393  0.704588  0.611217  0.424833   \n",
       "4   0.553892  0.764892  0.791278  0.781664  0.733397  0.544020  0.429115   \n",
       "5   0.545908  0.745957  0.779187  0.813230  0.753032  0.470840  0.439106   \n",
       "6   0.541317  0.879684  0.788107  0.938041  0.785627  0.389835  0.453378   \n",
       "7   0.566467  0.949507  0.765709  1.000000  0.793391  0.325728  0.480495   \n",
       "8   0.661876  0.667061  0.873538  0.993240  0.815464  0.293445  0.506660   \n",
       "9   0.666866  0.801578  0.921506  0.953908  0.878537  0.312841  0.545195   \n",
       "10  0.691417  0.901775  0.946283  0.869658  0.912672  0.330265  0.586584   \n",
       "11  0.693613  0.763708  0.961943  0.669982  0.933462  0.359590  0.635585   \n",
       "12  0.644511  0.734122  1.000000  0.580088  0.980943  0.391150  0.680304   \n",
       "13  0.526946  1.000000  0.909415  0.532097  1.000000  0.402196  0.731684   \n",
       "14  0.477844  0.900197  0.903865  0.476228  0.962207  0.382011  0.768316   \n",
       "15  0.422954  0.825247  0.896729  0.422258  0.940776  0.387402  0.800190   \n",
       "16  0.412575  0.814201  0.873538  0.436337  0.913699  0.414952  0.825880   \n",
       "17  0.436926  0.774359  0.868385  0.414939  0.876805  0.442107  0.849667   \n",
       "18  0.452096  0.785799  0.821407  0.377116  0.848252  0.485897  0.872027   \n",
       "19  0.462475  0.530572  0.788305  0.273311  0.852486  0.576698  0.902950   \n",
       "\n",
       "     Magno_Y   Magno_Z     Accel      Gyro     Magno  \n",
       "0   0.302746  0.106967  0.505830  0.127891  0.334348  \n",
       "1   0.284330  0.142153  0.522639  0.128037  0.190713  \n",
       "2   0.260420  0.179451  0.530832  0.111665  0.000000  \n",
       "3   0.235218  0.164673  0.501220  0.115441  0.047500  \n",
       "4   0.202262  0.128079  0.389446  0.145058  0.209829  \n",
       "5   0.176414  0.101337  0.391155  0.185615  0.366950  \n",
       "6   0.141519  0.066854  0.299389  0.320052  0.588005  \n",
       "7   0.097900  0.055595  0.179955  0.401276  0.801501  \n",
       "8   0.056220  0.070373  0.703714  0.422030  0.915206  \n",
       "9   0.023263  0.107671  0.732823  0.436930  1.000000  \n",
       "10  0.001616  0.178747  0.676972  0.414252  0.969607  \n",
       "11  0.000000  0.258269  0.824384  0.320156  0.931425  \n",
       "12  0.018740  0.325827  0.953320  0.290748  0.894140  \n",
       "13  0.042326  0.396200  0.490295  0.295560  0.883231  \n",
       "14  0.078191  0.492611  0.560222  0.290083  0.725974  \n",
       "15  0.109855  0.553132  0.639696  0.311387  0.678097  \n",
       "16  0.142488  0.601689  0.606253  0.278673  0.646768  \n",
       "17  0.176090  0.656580  0.617978  0.263885  0.597178  \n",
       "18  0.211632  0.693174  0.504112  0.269774  0.602620  \n",
       "19  0.242973  0.706545  0.679756  0.363115  0.750508  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def moving_average_dataframe(X, window_size):\n",
    "    X_new = pd.DataFrame()  # Create an empty DataFrame to store the results\n",
    "    for column in X.columns:\n",
    "        X_new[column] = X[column].rolling(window=window_size).mean()\n",
    "    # Drop rows with NaN values\n",
    "    X_new = X_new.dropna()\n",
    "    return X_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ma_not_normalized = moving_average_dataframe(df_not_normalized,5)\n",
    "ma_normalized_data = scaler.fit_transform(ma_not_normalized)\n",
    "ma_normalized = pd.DataFrame(ma_normalized_data, columns=ma_not_normalized.columns)\n",
    "ma_normalized.head(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
